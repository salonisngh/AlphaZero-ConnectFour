# ğŸ¯ AlphaZero Game AI

<div align="center">

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)
![Status](https://img.shields.io/badge/status-active-brightgreen.svg)

*A powerful two-player game AI inspired by DeepMind's AlphaZero*

**ğŸš€ Self-Play Learning â€¢ ğŸ§  Neural MCTS â€¢ ğŸ® Interactive Gaming**

</div>

---

## ğŸ“‹ Overview

This repository implements a **cutting-edge game-playing agent** that combines the power of Monte-Carlo Tree Search (MCTS) with deep neural networks. Through self-play, the AI discovers optimal strategies without any human game knowledge, achieving superhuman performance through pure reinforcement learning.

### ğŸ¯ Key Concept
> The agent plays **thousands of games against itself**, learning from each match to improve its neural network. This creates a virtuous cycle: better networks â†’ smarter tree search â†’ stronger gameplay â†’ better training data â†’ even better networks.

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ§  **Core AI Technology**
- ğŸŒ³ **Monte-Carlo Tree Search** with neural guidance
- ğŸ¯ **Self-Play Training** pipeline
- ğŸ“Š **Dual-headed network** (policy + value)
- ğŸ”„ **Iterative improvement** through experience

</td>
<td width="50%">

### ğŸ® **Game Support**
- ğŸ”´ **Connect Four** (6Ã—7 strategy game)
- ğŸ¯ **Pre-trained models** included
- ğŸ•¹ï¸ **Interactive Pygame interface**

</td>
</tr>
</table>

### ğŸš€ **Training Modes**

| Mode | ğŸ”„ Sequential | âš¡ Parallel |
|------|---------------|-------------|
| **Memory** | Low footprint | Higher usage |
| **Speed** | Standard | GPU optimized |
| **Use Case** | Limited resources | Modern hardware |
| **Debugging** | Easier | More complex |

---

## ğŸ® Games Gallery

<div>
### ğŸ”´ Connect Four
*Complex strategy showcasing deep learning capabilities*

```
ğŸ”´ğŸ”µâšªâšªâšªâšªâšª
ğŸ”´ğŸ”µâšªâšªâšªâšªâšª
ğŸ”µğŸ”´âšªâšªâšªâšªâšª
ğŸ”µğŸ”´âšªâšªâšªâšªâšª
ğŸ”´ğŸ”µâšªâšªâšªâšªâšª
ğŸ”µğŸ”´ğŸ”´ğŸ”´ğŸ”´âšªâšª
```

</div>

---



## ğŸ§  How It Works

<div align="center">

```mermaid
graph TD
    A[ğŸ® Self-Play Games] --> B[ğŸ“Š Training Data]
    B --> C[ğŸ§  Neural Network Training]
    C --> D[ğŸ¯ Better Policy & Value]
    D --> E[ğŸŒ³ Improved MCTS]
    E --> A
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
```

</div>

1. **ğŸ® Self-Play Generation**: AI plays against itself using MCTS + neural network
2. **ğŸ“Š Data Collection**: Records positions, move probabilities, and game outcomes
3. **ğŸ§  Network Training**: Learns to predict both move quality and position value
4. **ğŸ”„ Iterative Improvement**: Better network â†’ stronger MCTS â†’ better self-play

---

## ğŸ—ï¸ Architecture

<div >

### ğŸ§  Dual-Headed Neural Network
The neural network takes game board positions as input and outputs:
- **Policy head**: Probability distribution over legal moves
- **Value head**: Expected game outcome from current position

This dual-headed architecture allows the network to both guide move selection and evaluate positions during MCTS search.

</div>



---

## ğŸ“Š Performance Benchmarks

<div align="center">

### ğŸ† **Model Achievements**

| Game | ğŸ¯ Optimal Play | ğŸ¤– vs Random | ğŸ‘¤ vs Human |
|------|----------------|---------------|-------------|
| **ğŸ”´ Connect Four** | ğŸ¯ Near-optimal | >95% win rate | Beats intermediate players |

</div>

---

## ğŸ› ï¸ Dependencies

<div>

- **PyTorch** (`torch`) - Neural network training and inference
- **NumPy** (`numpy`) - Numerical computations
- **Matplotlib** (`matplotlib`) - Training visualization and plotting
- **tqdm** - Progress bars for training loops
- **Pygame** - Interactive Connect Four interface
- **Standard libraries**: `sys`, `math`, `random`

</div>

```bash
pip install torch numpy matplotlib pygame tqdm
```

---

## ğŸ¤ Contributing

<div align="center">

**Contributions are welcome! ğŸ‰**

| Area | Description |
|------|-------------|
| ğŸ® **New Games** | Add chess, checkers, or other games |
| âš¡ **Optimization** | Improve training speed and efficiency |
| ğŸ“Š **Visualization** | Enhanced monitoring and analysis tools| 
| ğŸŒ **Interface** | Web or mobile interfaces |

</div>

### ğŸ“ **Quick Contribution Guide**
1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch
3. ğŸ’» Make your changes
4. âœ… Add tests
5. ğŸ“¤ Submit a pull request

---

## ğŸ™ Acknowledgments

<div align="center">

**Inspired by groundbreaking research:**

ğŸ“š [AlphaGo Paper](https://www.nature.com/articles/nature16961) - *Mastering the game of Go with deep neural networks and tree search*

ğŸ“š [AlphaZero Paper](https://arxiv.org/abs/1712.01815) - *Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm*

</div>

---

<div align="center">

**â­ Star this repo if you found it helpful!**

![GitHub stars](https://img.shields.io/github/stars/Parthgogia/AlphaZero-ConnectFour?style=social)
![GitHub forks](https://img.shields.io/github/forks/Parthgogia/AlphaZero-ConnectFour?style=social)

</div>